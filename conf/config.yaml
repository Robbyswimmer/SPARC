exp:
  name: sparc_ppo_longctx
data:
  datasets: [narrativeqa, hotpotqa, triviaqa, nq_long]  # All available datasets
  dataset_order: [narrativeqa, hotpotqa, triviaqa, nq_long]  # From easiest to hardest
  curriculum_enabled: true  # Enable dataset curriculum
  qa_thresholds: [0.3, 0.4, 0.5]  # QA score thresholds to unlock next dataset
  curriculum_update_freq: 1000  # Check for curriculum updates every 1000 steps
  curriculum_window: 100  # Use 100-episode moving average for QA score
  split: train
env:
  chunk_size: 256
  max_window: 2048
curriculum:
  enabled: true
  start_chunks: 2
  max_chunks: 10
  growth_steps: 50_000
model:
  tokenizer: NousResearch/Meta-Llama-3.1-8B
  embed_dim: 128
  vocab_size: 128256
  net_arch: [{pi: [128, 64], vf: [128, 64]}]
train:
  n_envs: 8
  total_steps: 1_000_000
  lr: 3e-5
  n_steps: 1024
  batch_size: 256
  gamma: 0.99
  seed: 42  # Random seed for training
  gae_lambda: 0.95
  clip_range: 0.2
wandb:
  project: SPARC
eval:
  frequency: 10000   # Evaluate every 10k steps
  episodes: 20      # Number of episodes to evaluate on
  deterministic: true
  save_path: "./checkpoints/" # Directory to save best model checkpoints
  seed: 123  # Random seed for evaluation
  name_prefix: "sparc_best_model" # Prefix for the best model filename
