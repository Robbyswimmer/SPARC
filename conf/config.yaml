exp:
  name: sparc_ppo_longctx
data:
  datasets: [narrativeqa, hotpotqa, triviaqa, nq_long]  # All available datasets
  dataset_order: [narrativeqa, hotpotqa, triviaqa, nq_long]  # From easiest to hardest
  curriculum_enabled: true  # Enable dataset curriculum
  qa_thresholds: [0.3, 0.4, 0.5]  # QA score thresholds to unlock next dataset
  split: train
  dataset_config:  # Dataset-specific configuration
    narrativeqa:
      use_summaries: true  # Use summaries instead of full stories for NarrativeQA
    # Add other dataset-specific configurations as needed
env:
  chunk_size: 256
  max_window: 2048
  reward:
    alpha: 1.0            # Weight for QA score
    beta_keep: -0.01    # Cost per token KEPT in context window
    beta_compress: -0.05 # Cost per token in COMPRESSED chunk (if applicable)
    gamma_step: -0.01   # Flat cost applied per step, regardless of action
curriculum:
  enabled: true
  start_chunks: 2
  max_chunks: 10
  growth_steps: 50_000
model:
  tokenizer: NousResearch/Meta-Llama-3.1-8B
  embed_dim: 128
  vocab_size: 128256
  net_arch: [{pi: [128, 64], vf: [128, 64]}]
train:
  n_envs: 8
  total_steps: 1_000_000
  lr: 3e-5
  n_steps: 1024
  batch_size: 256
  gamma: 0.99
  seed: 42  # Random seed for training
  gae_lambda: 0.95
  clip_range: 0.2
wandb:
  project: SPARC
eval:
  frequency: 10000   # Evaluate every 10k steps
  episodes: 20      # Number of episodes to evaluate on
  deterministic: true
  save_path: "./checkpoints/" # Directory to save best model checkpoints
  seed: 123  # Random seed for evaluation
  name_prefix: "sparc_best_model" # Prefix for the best model filename
