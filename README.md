# SPARC

SPARC: Streaming Pruning And Retention Controller

SPARC is a lightweight reinforcement learning (RL) framework for dynamic, token-budgeted context management in frozen large language models (LLMs). Instead of using static heuristics like sliding windows or TF-IDF filtering, SPARC learns to decide in real time which incoming document chunks to KEEP, COMPRESS, or DROP, optimizing for downstream task accuracy under a fixed context window constraint.

üöÄ Motivation

LLMs are increasingly used in streaming applications‚Äîcode assistants, document QA systems, live meeting agents‚Äîbut are limited by their context window (e.g., 2K-8K tokens). Most current systems use simple heuristics that fail to adapt based on content relevance or task reward. SPARC introduces a trained policy that performs real-time memory management based on context utility and budget trade-offs.

üéØ Features

RL environment StreamingQAGym based on Gymnasium

Support for KEEP, DROP, and (soon) COMPRESS actions

PPO-based training with Stable-Baselines3

Pluggable reward function combining QA accuracy (EM + F1) and token cost

Frozen LLM inference via llama-cpp-python with 4-bit GGUF models

Fully streamable HF datasets (e.g., NarrativeQA, HotpotQA)

Lightweight summariser for COMPRESS action (WIP)

End-to-end train/evaluate scripts and WandB logging

‚öôÔ∏è Setup

# Clone and activate conda environment
- conda create -n streamqa python=3.11 -y
- conda activate streamqa
- pip install -r requirements.txt

Install llama-cpp-python for quantized LLM inference
- brew install cmake
- CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python

üß™ Quickstart

python src/scripts/example_demo.py   # random agent demo
python src/scripts/train.py          # PPO training loop
python src/scripts/evaluate.py       # accuracy, EM, token usage

üß† Key Concepts

Actions: {0: DROP, 1: KEEP, 2: COMPRESS (WIP)}

Reward: QA_EM + QA_F1 ‚àí token_penalty ‚àí step_penalty

Policy: 3-layer Transformer (<12M params)

üìù Citation

@inprogress{sparc2025,
  title={SPARC: Streaming Pruning and Retention Controller for Budget-Aware LLM Inference},
  author={Moseley, Robby},
  year={2025},
  journal={arXiv preprint},
  note={In preparation}
}

üìú License

MIT License. See LICENSE file for details.

ü§ù Contributing

We welcome PRs that improve training stability, extend evaluation tasks, or add summarisation capabilities. Please lint and test your code and follow the contribution guidelines in .windsurfrules.
